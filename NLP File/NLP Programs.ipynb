{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2887aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "The Natural Language Toolkit (NLTK) is an open source Python library for Natural Language \n",
      "Processing.  A free online book is available. If you use the library for academic research, please\n",
      "cite the book.\n",
      "\n",
      "Sentence Tokenize:\n",
      "['The Natural Language Toolkit (NLTK) is an open source Python library for Natural Language \\nProcessing.', 'A free online book is available.', 'If you use the library for academic research, please\\ncite the book.']\n",
      "\n",
      "Word Tokenize:\n",
      "['The', 'Natural', 'Language', 'Toolkit', '(', 'NLTK', ')', 'is', 'an', 'open', 'source', 'Python', 'library', 'for', 'Natural', 'Language', 'Processing', '.', 'A', 'free', 'online', 'book', 'is', 'available', '.', 'If', 'you', 'use', 'the', 'library', 'for', 'academic', 'research', ',', 'please', 'cite', 'the', 'book', '.']\n"
     ]
    }
   ],
   "source": [
    "# program 1 (Tokenization)\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "text = '''The Natural Language Toolkit (NLTK) is an open source Python library for Natural Language \n",
    "Processing.  A free online book is available. If you use the library for academic research, please\n",
    "cite the book.'''\n",
    "sent = sent_tokenize(text)\n",
    "word = word_tokenize(text)\n",
    "print('Original Text:')\n",
    "print(text)\n",
    "print('\\nSentence Tokenize:')\n",
    "print(sent)\n",
    "print('\\nWord Tokenize:')\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "420f837c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'toolkit', '(', 'nltk', ')', 'open', 'source', 'python', 'library', 'natural', 'language', 'processing', '.', 'free', 'online', 'book', 'available', '.', 'use', 'library', 'academic', 'research', ',', 'please', 'cite', 'book', '.']\n"
     ]
    }
   ],
   "source": [
    "# program 2 (Stopwords)\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = '''The Natural Language Toolkit (NLTK) is an open source Python library for Natural Language \n",
    "Processing.  A free online book is available. If you use the library for academic research, please\n",
    "cite the book.'''\n",
    "word = word_tokenize(text)\n",
    "stop_word = stopwords.words('english')\n",
    "filter_word = []\n",
    "\n",
    "for i in [x.lower() for x in word]:\n",
    "    if i not in stop_word:\n",
    "        filter_word.append(i)\n",
    "print(filter_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8ca0ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# program 3 (Stemming)\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f84f1464",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09e57aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running : run\n",
      "quickly : quickli\n",
      "programming : program\n",
      "foxes : fox\n",
      "happily : happili\n"
     ]
    }
   ],
   "source": [
    "words = ['running','quickly','programming','foxes','happily']\n",
    "for i in words:\n",
    "    print(i,':',ps.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79cb15a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# program 4 (Lemmatization)\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "136544b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9eecb999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books : book\n",
      "goes : go\n",
      "programming : programming\n",
      "foxes : fox\n",
      "scientific : scientific\n"
     ]
    }
   ],
   "source": [
    "words = ['books','goes','programming','foxes','scientific']\n",
    "for i in words:\n",
    "    print(i,':',ls.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "906fb299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['support@example.com', 'sales@company123.com.', 'feedback@emailco.net.', 'marketing.team@yourcompany.org.', 'info@website1234.org.']\n"
     ]
    }
   ],
   "source": [
    "# program 5 (Regular expression emails retrival)\n",
    "import re\n",
    "text = '''Please contact our support team at support@example.com for any assistance. If you have \n",
    "questions about your order, email sales@company123.com. We're excited to hear your feedback at \n",
    "feedback@emailco.net. Reach out to our marketing department at marketing.team@yourcompany.org. \n",
    "For general inquiries, email info@website1234.org.'''\n",
    "lst = re.findall('\\S+@\\S+',text)\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d860afe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7', '9', '3', '4', '5', '2', '8', '1', '5', '9']\n"
     ]
    }
   ],
   "source": [
    "# program 6 (Regular expression numbers retrival)\n",
    "import re\n",
    "text = '''In a survey, 7 out of 9 people prefer using smartphones for communication, while 3 out \n",
    "of 4 still use traditional landlines. The average age of the participants was 5, and 2 in 8 were \n",
    "under 1 years old. We observed a 5% increase in web traffic this month, and the company's stock \n",
    "price rose by 9%.'''\n",
    "lst = re.findall('[0-9]+',text)\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "836be5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 ['The', 'sun', 'was', 'setting', 'over', 'the', 'horizon', ';', 'its', 'warm', ',', 'golden', 'glow', 'painted', 'the', 'sky', 'in', 'shades', 'of', 'orange', 'and', 'pink', '.']\n",
      "Sentence 2 ['She', 'couldn', \"'\", 't', 'believe', 'her', 'luck', 'as', 'she', 'held', 'the', 'winning', 'lottery', 'ticket', ',', 'worth', 'a', 'million', 'dollars', '!']\n",
      "Sentence 3 ['Despite', 'his', 'initial', 'reservations', ',', 'John', 'decided', 'to', 'accept', 'the', 'job', 'offer', 'in', 'New', 'York', 'City', ';', 'he', 'was', 'excited', 'about', 'the', 'new', 'opportunities', 'it', 'would', 'bring', '.']\n"
     ]
    }
   ],
   "source": [
    "# program 7 (Word Punctuation)\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "sent = [\"The sun was setting over the horizon; its warm, golden glow painted the sky in shades of orange and pink.\",\n",
    "\"She couldn't believe her luck as she held the winning lottery ticket, worth a million dollars!\",\n",
    "\"Despite his initial reservations, John decided to accept the job offer in New York City; he was excited about the new opportunities it would bring.\"]\n",
    "tokenizor = WordPunctTokenizer()\n",
    "tokenized_sent = [tokenizor.tokenize(sentence) for sentence in sent]\n",
    "for i,tokens in enumerate(tokenized_sent):\n",
    "    print(f'Sentence {i+1}',tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea03bf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Word: 41600\n",
      "\n",
      "Top 10 most common words:\n",
      "[('.', 94687), (',', 72360), ('the', 58251), ('of', 35979), ('to', 34035), ('in', 26478), ('said', 25224), ('and', 25043), ('a', 23492), ('mln', 18037)]\n",
      "\n",
      "Top 10 most common bigrams:\n",
      "[((',', '000'), 10266), ((\"'\", 's'), 9220), (('lt', ';'), 8693), (('&', 'lt'), 8688), (('.', 'The'), 8530), (('said', '.'), 7888), (('of', 'the'), 6803), (('in', 'the'), 6487), (('U', '.'), 6350), (('.', 'S'), 5833)]\n",
      "\n",
      "Top 10 most common trigrams:\n",
      "[(('&', 'lt', ';'), 8687), (('U', '.', 'S'), 5693), (('.', 'S', '.'), 5360), ((',', '000', 'vs'), 2577), (('the', 'U', '.'), 1959), ((',', '000', 'dlrs'), 1524), (('said', '.', 'The'), 1516), (('.', '5', 'mln'), 1337), (('he', 'said', '.'), 1229), ((',', '000', 'Revs'), 1198)]\n"
     ]
    }
   ],
   "source": [
    "# program 8 (Monolingual Corpus)\n",
    "import nltk\n",
    "from nltk.util import bigrams,trigrams\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "corpus_word = reuters.words()\n",
    "corpus_length = len(corpus_word)\n",
    "\n",
    "word_freq = FreqDist(corpus_word)\n",
    "\n",
    "bigram_freq = FreqDist(list(bigrams(corpus_word)))\n",
    "trigram_freq = FreqDist(list(trigrams(corpus_word)))\n",
    "\n",
    "distinct_word = len(word_freq)\n",
    "print(f'Distinct Word: {distinct_word}')\n",
    "\n",
    "print('\\nTop 10 most common words:')\n",
    "print(word_freq.most_common(10))\n",
    "\n",
    "print('\\nTop 10 most common bigrams:')\n",
    "print(bigram_freq.most_common(10))\n",
    "\n",
    "print('\\nTop 10 most common trigrams:')\n",
    "print(trigram_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "781bcf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT')]\n",
      "[('Natural', 'JJ')]\n",
      "[('Language', 'NN')]\n",
      "[('Toolkit', 'NN')]\n",
      "[('(', '('), ('NLTK', 'NNP'), (')', ')')]\n",
      "[('is', 'VBZ')]\n",
      "[('an', 'DT')]\n",
      "[('open', 'JJ')]\n",
      "[('source', 'NN')]\n",
      "[('Python', 'NN')]\n",
      "[('library', 'NN')]\n",
      "[('for', 'IN')]\n",
      "[('Natural', 'JJ')]\n",
      "[('Language', 'NN')]\n",
      "[('Processing', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# program 9 (POS Tags)\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = 'The Natural Language Toolkit (NLTK) is an open source Python library for Natural Language Processing.'\n",
    "for i in text.split():\n",
    "    print(pos_tag(word_tokenize(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff8f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
